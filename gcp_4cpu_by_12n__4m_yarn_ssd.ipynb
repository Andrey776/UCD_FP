{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17f41b1c",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7480aef",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-24T20:49:44.219Z"
    }
   },
   "outputs": [],
   "source": [
    "#import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pyspark\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from pyspark import SparkContext, SQLContext\n",
    "\n",
    "from math import sqrt\n",
    "from time import time as ttt\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier as DTC_spark\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "import joblib\n",
    "from joblib import parallel_backend\n",
    "from joblib import Parallel, delayed\n",
    "from joblib import parallel_backend\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33b542c",
   "metadata": {},
   "source": [
    "# Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a5c4ff",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-24T20:49:44.220Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"Yarn\").appName(\"spark_app_1234\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "spark\n",
    "d0 = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"csv\") \n",
    "    .option(\"header\",\"true\") \n",
    "    .option(\"inferSchema\",\"true\") \n",
    "    .load(\"gs://mas-a5-storage-1/notebooks/jupyter/application_train.csv\")\n",
    ")\n",
    "\n",
    "d1 = d0.filter(d0.DAYS_EMPLOYED != 365243).select('TARGET','DAYS_EMPLOYED')\n",
    "\n",
    "print(d1.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f60150",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-24T20:49:44.222Z"
    }
   },
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333b63a3",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ec74a5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-24T20:49:44.223Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_spark_data(n_part, k_mult):\n",
    "    '''\n",
    "    takes 'DAYS_EMPLOYED and 'TARGET'  from d1 (alreay filtered)\n",
    "    rearrane in n partitions (if n==0 keeps initial number of partitions)\n",
    "    prints final shape/ partition\n",
    "    returns d2 - spark df\n",
    "    '''\n",
    "    data = d1\n",
    "    data_new = data # first step in the cycle\n",
    "    for i in range(k_mult-1):\n",
    "        data_tmp = data.select('TARGET', \\\n",
    "                     f.col('DAYS_EMPLOYED')*(f.lit(0.9995) + f.rand()/1000)).\\\n",
    "                   toDF('TARGET','DAYS_EMPLOYED')\n",
    "        data_tmp = data_tmp.select('TARGET', f.floor('DAYS_EMPLOYED'))\n",
    "        data_new = data_new.union(data_tmp)\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=[\"DAYS_EMPLOYED\"], \n",
    "                        outputCol=\"DAYS_EMPLOYED_vect\")\n",
    "    d2 = assembler.transform(data_new)\n",
    "    if n_part != 0:\n",
    "        d2 = d2.repartition(n_part, \"DAYS_EMPLOYED_vect\")       \n",
    "    #print(f'n-partitions initial: {d2.rdd.getNumPartitions()}; df size: {d2.count()}\\n')\n",
    "\n",
    "    return d2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e62426a",
   "metadata": {},
   "source": [
    "# Params an run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c383b6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-24T19:59:26.116Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## !! mind the fn NOT TO rewrite results\n",
    "fn = '4cpu_by_12n__4m_yarn_ssd_repart'\n",
    "size_mult = [1, 2, 5, 10, 15, 20, 30, 50, 100]\n",
    "partitions = [8, 12, 16, 24, 36, 72] \n",
    "# initial, 2**i incl n_nodes till n_cpu*n_nodes and n_cpu*n_nodes\n",
    "n_iter = 3\n",
    "\n",
    "print('d1-size', d1.count())\n",
    "n_part_base = d1.rdd.getNumPartitions()\n",
    "print('n-partitions initial', n_part_base, '\\n')\n",
    "\n",
    "rd1 = {}\n",
    "for k_size_mult in size_mult:\n",
    "    rd2 = {}\n",
    "    for n_part in partitions:\n",
    "        df = prepare_spark_data(n_part, k_size_mult)\n",
    "        df.cache()\n",
    "        print('======================================================================')\n",
    "        print(f'=== size_mult={k_size_mult}; \\\n",
    "        n_part_req={n_part}; n_part_act={df.rdd.getNumPartitions()}; df_count={df.count()} ===\\n')\n",
    "        times = [0 for i in range(n_iter)]\n",
    "        models = {}\n",
    "        for i in range(n_iter):\n",
    "            dt = DTC_spark(labelCol=\"TARGET\",\n",
    "                       featuresCol=\"DAYS_EMPLOYED_vect\",\n",
    "                       minInfoGain=0.0001,\n",
    "                       impurity='entropy',\n",
    "                       maxDepth=14, maxBins=2**14, # it differs from scikit learn - it means number of canidate split points\n",
    "                       #minInstancesPerNode = 1,\n",
    "                       #checkpointInterval = 10\n",
    "                       )\n",
    "           \n",
    "            t0 = ttt()\n",
    "            model = dt.fit(df)\n",
    "            t1 = ttt()\n",
    "            times[i] = t1-t0\n",
    "            models[i] = model \n",
    "            print('model', i, 'build time', round(times[i],2), '\\n', model)\n",
    "        rd2[n_part] = (models, times)\n",
    "    rd1[k_size_mult] = rd2\n",
    "\n",
    "result = rd1  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6b1aef",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c2c809",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T20:45:34.029733Z",
     "start_time": "2021-11-24T20:45:33.162166Z"
    }
   },
   "outputs": [],
   "source": [
    "di = result\n",
    "df_res_time = pd.DataFrame()\n",
    "df_res_nodes = pd.DataFrame()\n",
    "for p1 in di.keys():\n",
    "    for p2 in di[p1].keys():\n",
    "        df_res_time.loc[p1, p2] = np.round(np.median(di[p1][p2][1]), 1)\n",
    "        m_tmp = di[p1][p2][0]\n",
    "        n_nodes = []\n",
    "        for i,_ in enumerate(m_tmp):\n",
    "            tmp = f'{m_tmp[i]}'.split(' ')\n",
    "            print(i, tmp)\n",
    "            n_nodes.append([int(x.split('=')[1][:-1]) for x in tmp if x[:4]== 'numN'][0])\n",
    "        df_res_nodes.loc[p1, p2] = np.round(np.mean(n_nodes), 1)\n",
    "\n",
    "df_res_time.to_csv(f'gs://mas-a5-storage-1/notebooks/jupyter/obj/{fn}_t.csv')\n",
    "df_res_nodes.to_csv(f'gs://mas-a5-storage-1/notebooks/jupyter/obj/{fn}_n.csv')\n",
    "\n",
    "display(df_res_time)      \n",
    "display(df_res_nodes)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7dc76e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T20:46:08.743543Z",
     "start_time": "2021-11-24T20:46:08.659177Z"
    }
   },
   "outputs": [],
   "source": [
    "#fn = '4cpu_by_8n__4m_yarn_ssd'\n",
    "tmp = pd.read_csv(f'gs://mas-a5-storage-1/notebooks/jupyter/obj/{fn}_t.csv')\n",
    "print(fn)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3dc9e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T20:46:10.555126Z",
     "start_time": "2021-11-24T20:46:10.506268Z"
    }
   },
   "outputs": [],
   "source": [
    "#fn = '4cpu_by_8n__4m_yarn_ssd'\n",
    "tmp = pd.read_csv(f'gs://mas-a5-storage-1/notebooks/jupyter/obj/{fn}_n.csv')\n",
    "print(fn)\n",
    "tmp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
