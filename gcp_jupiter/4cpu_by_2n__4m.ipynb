{"cells":[{"cell_type":"code","execution_count":1,"id":"f7480aef","metadata":{"ExecuteTime":{"end_time":"2021-11-22T20:08:47.705416Z","start_time":"2021-11-22T20:08:46.874095Z"}},"outputs":[],"source":["#import sys\n","import pandas as pd\n","import numpy as np\n","import random\n","import pyspark\n","import itertools\n","import matplotlib.pyplot as plt\n","import itertools\n","import seaborn as sns\n","import pickle\n","import statsmodels.api as sm\n","\n","from pyspark import SparkContext, SQLContext\n","\n","from math import sqrt\n","from time import time as ttt\n","\n","from pyspark.sql import SparkSession\n","from pyspark.sql import functions as f\n","\n","from pyspark.ml.linalg import Vectors\n","from pyspark.ml.feature import VectorAssembler\n","from pyspark.ml import Pipeline\n","from pyspark.ml.classification import DecisionTreeClassifier as DTC_spark\n","from pyspark.ml.feature import StringIndexer, VectorIndexer\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","\n","import joblib\n","from joblib import parallel_backend\n","from joblib import Parallel, delayed\n","from joblib import parallel_backend\n"]},{"cell_type":"code","execution_count":2,"id":"96a5c4ff","metadata":{"ExecuteTime":{"end_time":"2021-11-22T20:09:04.342903Z","start_time":"2021-11-22T20:08:47.710778Z"}},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["2\n"]}],"source":["spark = SparkSession.builder.master(\"local\").appName(\"spark_app_1234\").getOrCreate()\n","sc = spark.sparkContext\n","spark\n","d0 = (\n","    spark\n","    .read\n","    .format(\"csv\") \n","    .option(\"header\",\"true\") \n","    .option(\"inferSchema\",\"true\") \n","    .load(\"gs://mas-a5-storage-1/notebooks/jupyter/application_train.csv\")\n",")\n","\n","d1 = d0.filter(d0.DAYS_EMPLOYED != 365243).select('TARGET','DAYS_EMPLOYED')\n","\n","print(d1.rdd.getNumPartitions())"]},{"cell_type":"code","execution_count":3,"id":"e0ec74a5","metadata":{"ExecuteTime":{"end_time":"2021-11-22T20:09:04.354105Z","start_time":"2021-11-22T20:09:04.347962Z"}},"outputs":[],"source":["def prepare_spark_data(n_part, k_mult):\n","    '''\n","    takes 'DAYS_EMPLOYED and 'TARGET'  from d1 (alreay filtered)\n","    rearrane in n partitions (if n=='base' keeps initial number of partitions)\n","    prints final shape/ partition\n","    returns nothing, but spark d2 is ready\n","    '''\n","    data = d1\n","    data_new = data # first step in the cycle\n","    for i in range(k_mult-1):\n","        data_tmp = data.select('TARGET', \\\n","                     f.col('DAYS_EMPLOYED')*(f.lit(0.9995) + f.rand()/1000)).\\\n","                   toDF('TARGET','DAYS_EMPLOYED')\n","        data_tmp = data_tmp.select('TARGET', f.floor('DAYS_EMPLOYED'))\n","        data_new = data_new.union(data_tmp)\n","\n","    assembler = VectorAssembler(inputCols=[\"DAYS_EMPLOYED\"], \n","                        outputCol=\"DAYS_EMPLOYED_vect\")\n","    d2 = assembler.transform(data_new)\n","    if n_part != 0:\n","        d2 = df.repartition(n_part)       \n","    #print(f'n-partitions initial: {d2.rdd.getNumPartitions()}; df size: {d2.count()}\\n')\n","\n","    return d2"]},{"cell_type":"markdown","id":"5e62426a","metadata":{},"source":["# Params an run"]},{"cell_type":"code","execution_count":null,"id":"62c383b6","metadata":{"ExecuteTime":{"start_time":"2021-11-22T20:09:36.414Z"},"scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["d1-size 252137\n","n-partitions initial 2 \n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["=== size_mult=1; n_part=0; df_count=252137 ===\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["model 0 build time 12.813473463058472 \n"," DecisionTreeClassificationModel: uid=DecisionTreeClassifier_0e724859a07d, depth=14, numNodes=123, numClasses=2, numFeatures=1\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["model 1 build time 8.540323734283447 \n"," DecisionTreeClassificationModel: uid=DecisionTreeClassifier_a457c48b6971, depth=14, numNodes=123, numClasses=2, numFeatures=1\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["=== size_mult=1; n_part=2; df_count=252137 ===\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["model 0 build time 8.21955394744873 \n"," DecisionTreeClassificationModel: uid=DecisionTreeClassifier_ab46eac2d4cb, depth=14, numNodes=123, numClasses=2, numFeatures=1\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["model 1 build time 7.032540798187256 \n"," DecisionTreeClassificationModel: uid=DecisionTreeClassifier_6aa5692fb1c2, depth=14, numNodes=123, numClasses=2, numFeatures=1\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["=== size_mult=2; n_part=0; df_count=504274 ===\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["model 0 build time 13.713611125946045 \n"," DecisionTreeClassificationModel: uid=DecisionTreeClassifier_db26cd062a47, depth=14, numNodes=147, numClasses=2, numFeatures=1\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["model 1 build time 12.56681776046753 \n"," DecisionTreeClassificationModel: uid=DecisionTreeClassifier_37400c318c37, depth=14, numNodes=147, numClasses=2, numFeatures=1\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["=== size_mult=2; n_part=2; df_count=504274 ===\n","\n"]}],"source":["fn = '4cpu_by_2n__4m'\n","size_mult = [1, 2]\n","partitions = [0, 2] # basic one, number of clusters, number of CPU\n","n_iter = 2\n","\n","print('d1-size', d1.count())\n","n_part_base = d1.rdd.getNumPartitions()\n","print('n-partitions initial', n_part_base, '\\n')\n","\n","rd1 = {}\n","for k_size_mult in size_mult:\n","    rd2 = {}\n","    for n_part in partitions:\n","        df = prepare_spark_data(n_part, k_size_mult)\n","        df.cache()\n","        print(f'=== size_mult={k_size_mult}; n_part={n_part}; df_count={df.count()} ===\\n')\n","        times = [0 for i in range(n_iter)]\n","        models = {}\n","        for i in range(n_iter):\n","            dt = DTC_spark(labelCol=\"TARGET\",\n","                       featuresCol=\"DAYS_EMPLOYED_vect\",\n","                       minInfoGain=0.0001,\n","                       impurity='entropy',\n","                       maxDepth=14, maxBins=2**14, # it differs from scikit learn - it means number of canidate split points\n","                       #minInstancesPerNode = 1,\n","                       #checkpointInterval = 10\n","                       )\n","           \n","            t0 = ttt()\n","            model = dt.fit(df)\n","            t1 = ttt()\n","            times[i] = t1-t0\n","            models[i] = model \n","            print('model', i, 'build time', times[i], '\\n', model)\n","        rd2[n_part] = (models, times)\n","    rd1[k_size_mult] = rd2\n","\n","result = rd1  "]},{"cell_type":"code","execution_count":null,"id":"91c2c809","metadata":{"ExecuteTime":{"end_time":"2021-11-22T20:07:39.610681Z","start_time":"2021-11-22T20:07:39.064248Z"}},"outputs":[],"source":["di = result\n","df_res_time = pd.DataFrame()\n","df_res_nodes = pd.DataFrame()\n","for p1 in di.keys():\n","    for p2 in di[p1].keys():\n","        df_res_time.loc[p1, p2] = np.round(np.mean(di[p1][p2][1]), 1)\n","        m_tmp = di[p1][p2][0]\n","        n_nodes = []\n","        for i,_ in enumerate(m_tmp):\n","            tmp = f'{m_tmp[i]}'.split(' ')\n","            print(i, tmp)\n","            n_nodes.append([int(x.split('=')[1][:-1]) for x in tmp if x[:4]== 'numN'][0])\n","        df_res_nodes.loc[p1, p2] = np.round(np.mean(n_nodes), 1)\n","\n","df_res_time.to_csv(f'gs://mas-a5-storage-1/notebooks/jupyter/obj/{fn}_t.csv')\n","df_res_nodes.to_csv(f'gs://mas-a5-storage-1/notebooks/jupyter/obj/{fn}_n.csv')\n","\n","display(df_res_time)      \n","display(df_res_nodes)     "]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":5}